{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b4d6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import nltk\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343879b7",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dcd18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', index_col=0)\n",
    "X_train = data[['title', 'author', 'text']]\n",
    "y_train = data[['label']]\n",
    "X_test = pd.read_csv('test.csv', index_col=0)\n",
    "y_test = pd.read_csv('submit.csv', index_col=0)\n",
    "# Transform class names\n",
    "y_test[y_test['label'] == 1] = 'Fake'\n",
    "y_test[y_test['label'] == 0] = 'Truth'\n",
    "y_train[y_train['label'] == 1] = 'Fake'\n",
    "y_train[y_train['label'] == 0] = 'Truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcb63ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "20800    Specter of Trump Loosens Tongues, if Not Purse...\n",
       "20801    Russian warships ready to strike terrorists ne...\n",
       "20802    #NoDAPL: Native American Leaders Vow to Stay A...\n",
       "20803    Tim Tebow Will Attempt Another Comeback, This ...\n",
       "20804    Keiser Report: Meme Wars (E995) 42 mins ago 1 ...\n",
       "                               ...                        \n",
       "25995    The Bangladeshi Traffic Jam That Never Ends - ...\n",
       "25996    John Kasich Signs One Abortion Bill in Ohio bu...\n",
       "25997    California Today: What, Exactly, Is in Your Su...\n",
       "25998    300 US Marines To Be Deployed To Russian Borde...\n",
       "25999    Awkward Sex, Onscreen and Off - The New York T...\n",
       "Length: 5200, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['title'] + ' ' + X_test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b16aa51",
   "metadata": {},
   "source": [
    "# Create pre-processing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8e27c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, x):\n",
    "        x['summary'] = x['title'] + ' ' + x['text']\n",
    "        x['summary'] = x['summary'].map(Transformer.__remove_punctuation)\n",
    "        x['summary'] = x['summary'].map(Transformer.__clean_text)\n",
    "        x['summary'] = x['summary'].map(Transformer.__lemmatizer)\n",
    "        x['summary'] = x['summary'].map(Transformer.__remove_stopwords)\n",
    "\n",
    "        return x['summary']\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def __clean_text(text):\n",
    "        text= text.lower()\n",
    "        text= ' '.join(re.sub(\"(@[A-Za-z0-9]+)\", \" \", text).split()) #tags\n",
    "        text= ' '.join(re.sub(\"^@?(\\w){1,15}$\", \" \", text).split())\n",
    "\n",
    "        text= ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", text).split())   #Links\n",
    "        text= ' '.join(re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\" \", text).split()) \n",
    "        text= ' '.join(re.sub(r'http\\S+', '',text).split())\n",
    "        text= ' '.join(re.sub(r'www\\S+', '',text).split())\n",
    "        text= ' '.join(re.sub(\"\\s+\", \" \",text).split()) #Extrem white Space\n",
    "        text= ' '.join(re.sub(\"[^-9A-Za-z ]\", \"\" ,text).split()) #digits \n",
    "        text= ' '.join(re.sub('-', ' ', text).split()) \n",
    "        text= ' '.join(re.sub('_', ' ', text).split()) #underscore     \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def __remove_stopwords(text):\n",
    "        \"\"\"The function to removing stopwords\"\"\"\n",
    "        stop = stopwords.words('english')\n",
    "        text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "        return \" \".join(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __lemmatizer(text):\n",
    "        \"\"\"The function to apply lemmatizing\"\"\"\n",
    "        word_list = nltk.word_tokenize(text)\n",
    "        lemmatized_text = ' '.join([WordNetLemmatizer().lemmatize(w) for w in word_list])\n",
    "        return lemmatized_text\n",
    "    \n",
    "    @staticmethod\n",
    "    def __remove_punctuation(text):\n",
    "        \"\"\"The function to remove punctuation\"\"\"\n",
    "        text = str(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        return text.translate(table)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6259724",
   "metadata": {},
   "source": [
    "# Fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda75e6",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "319b4365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 <__main__.Transformer object at 0x000002098C2E7340>),\n",
       "                ('countvectorizer', CountVectorizer()),\n",
       "                ('tfidftransformer', TfidfTransformer()),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(max_depth=200, n_estimators=400))])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit new model\n",
    "model_RF = make_pipeline(Transformer(), CountVectorizer(), TfidfTransformer(), RandomForestClassifier())\n",
    "model_RF = GridSearchCV(model_RF, {'randomforestclassifier__max_depth': [10, 20, 50, 100, 200],\n",
    "                                   'randomforestclassifier__n_estimators': [100, 200, 400]}).fit(X_train, y_train).best_estimator_\n",
    "model_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "144e0134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/RF.joblib']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump model\n",
    "joblib.dump(model_RF, 'models/RF.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bcc0f353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 <__main__.Transformer object at 0x00000208D6AA8A90>),\n",
       "                ('countvectorizer', CountVectorizer()),\n",
       "                ('tfidftransformer', TfidfTransformer()),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(max_depth=200, n_estimators=400))])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fitted model\n",
    "model_RF = joblib.load('models/RF.joblib')\n",
    "model_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad3609",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3ca83a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 <__main__.Transformer object at 0x00000208E019A790>),\n",
       "                ('countvectorizer', CountVectorizer()),\n",
       "                ('tfidftransformer', TfidfTransformer()),\n",
       "                ('logisticregression', LogisticRegression(C=35))])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit new model\n",
    "model_LR = make_pipeline(Transformer(), CountVectorizer(), TfidfTransformer(), LogisticRegression(C=35)).fit(X_train, y_train)\n",
    "#model_LR = GridSearchCV(model_LR, {'logisticregression__C': [0.1, 0.5, 1, 3, 7, 15, 20, 35, 50]}).fit(X_train, y_train).best_estimator_\n",
    "model_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f42095c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/LR.joblib']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump model\n",
    "joblib.dump(model_LR, 'models/LR.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dbc5d453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 <__main__.Transformer object at 0x00000208E019A0D0>),\n",
       "                ('countvectorizer', CountVectorizer()),\n",
       "                ('tfidftransformer', TfidfTransformer()),\n",
       "                ('logisticregression', LogisticRegression(C=35))])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fitted model\n",
    "model_LR = joblib.load('models/LR.joblib')\n",
    "model_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432d641",
   "metadata": {},
   "source": [
    "# Models testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1371b",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "58a70e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6023076923076923"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_NB = model_NB.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ec12c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.67      0.54      0.60      2861\n",
      "       Truth       0.55      0.68      0.61      2339\n",
      "\n",
      "    accuracy                           0.60      5200\n",
      "   macro avg       0.61      0.61      0.60      5200\n",
      "weighted avg       0.62      0.60      0.60      5200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13016609",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e142f36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6375"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_SVC = model_SVC.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "80b2dbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.69      0.63      0.65      2861\n",
      "       Truth       0.59      0.65      0.62      2339\n",
      "\n",
      "    accuracy                           0.64      5200\n",
      "   macro avg       0.64      0.64      0.64      5200\n",
      "weighted avg       0.64      0.64      0.64      5200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_SVC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70c93d",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fc526e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6784615384615384"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_RF = model_RF.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "62c6449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.74      0.64      0.69      2861\n",
      "       Truth       0.62      0.73      0.67      2339\n",
      "\n",
      "    accuracy                           0.68      5200\n",
      "   macro avg       0.68      0.68      0.68      5200\n",
      "weighted avg       0.69      0.68      0.68      5200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d4b04",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "21b4914a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6388461538461538"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_LR = model_LR.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "aeb1e045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.69      0.63      0.66      2861\n",
      "       Truth       0.59      0.65      0.62      2339\n",
      "\n",
      "    accuracy                           0.64      5200\n",
      "   macro avg       0.64      0.64      0.64      5200\n",
      "weighted avg       0.64      0.64      0.64      5200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e7de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9b90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
